{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "  \n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw_data = f.read(10000) \n",
    "    return chardet.detect(raw_data)[\"encoding\"]\n",
    "\n",
    "def extract_description(file_path):\n",
    "   \n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)  # Detect encoding\n",
    "        with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        match = re.search(r\"<DESCRIPTION>(.*?)</DESCRIPTION>\", content, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    glove_dict = {}\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector\n",
    "    return glove_dict\n",
    "\n",
    "\n",
    "def get_glove_embeddings(words, glove_dict, embedding_dim):\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        embeddings.append(glove_dict.get(word, np.zeros(embedding_dim)))  # Return zero vector if not found\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text.lower() for token in doc if token.is_alpha]  # Keep only words\n",
    "\n",
    "def sentences_to_glove_embedding(descs,glove_dict):\n",
    "\n",
    "    embedding_dataset=[]\n",
    "    for sentence in descs:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "\n",
    "        filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        sentence=' '.join(filtered_words)\n",
    "        # print(sentence)\n",
    "\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        \n",
    "        # tokens=tokens[:tokens_to_take]\n",
    "        \n",
    "        word_embeddings = get_glove_embeddings(tokens, glove_dict, 300)\n",
    "        embedding_dataset.append(word_embeddings)\n",
    "\n",
    "    summed_embedding = [np.sum(sample, axis=0) for sample in embedding_dataset]\n",
    "    summed_embedding_array=np.array(summed_embedding)\n",
    "\n",
    "    return summed_embedding_array\n",
    "\n",
    "\n",
    "\n",
    "def process_text_file_to_get_embedding(base_dir,source_file):\n",
    "    sentences=[]\n",
    "    files_not_found=[]\n",
    "    with open(source_file,\"r\") as file:\n",
    "        for i,line in enumerate(file):\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                actual_path=f\"{base_dir}{line}.eng\"\n",
    "            \n",
    "            desc = extract_description(actual_path)\n",
    "            if desc is not None:\n",
    "                sentences.append(desc)\n",
    "            else:\n",
    "                files_not_found.append(i)\n",
    "\n",
    "    \n",
    "    # print(len(sentences))\n",
    "    glove_file = \"glove.6B/glove.6B.300d.txt\"\n",
    "    glove_dict = load_glove_model(glove_file)\n",
    "\n",
    "    return sentences_to_glove_embedding(sentences,glove_dict),files_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_svm_format(features, labels, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for label_indices, feature_vector in zip(labels, features):\n",
    "            label_str = ','.join(map(str, label_indices))  # Labels\n",
    "            feature_str = ' '.join([f\"{i}:{v:.6f}\" for i, v in enumerate(feature_vector) if v != 0])            \n",
    "            f.write(f\"{label_str} {feature_str}\\n\")\n",
    "\n",
    "def one_hot_to_indices(one_hot_labels):\n",
    "    \"\"\"Convert one-hot encoding to label indices.\"\"\"\n",
    "    return [list(np.where(row == 1)[0]) for row in one_hot_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"iaprtc12/annotations_complete_eng/\"\n",
    "train_file=\"IAPRTC/iapr_train_list.txt\"\n",
    "\n",
    "\n",
    "train_summed_array,files_not_found=process_text_file_to_get_embedding(base_dir,train_file)\n",
    "print(train_summed_array.shape)\n",
    "\n",
    "files_not_found = np.array(files_not_found)\n",
    "print(len(files_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_data = loadmat(\"IAPRTC/IAPRTC-12_TrainLabels.mat\")\n",
    "I_z_tr=train_label_data['I_z_tr']\n",
    "label_column_1 = I_z_tr[0,0]  \n",
    "label_column_2 = I_z_tr[0, 1] \n",
    "\n",
    "print(\"Column 1 Shape:\", label_column_1.shape) \n",
    "    \n",
    "# label_train_column_1=label_column_1[:2734]\n",
    "mask = np.ones(label_column_1.shape[0], dtype=bool)\n",
    "mask[files_not_found] = False\n",
    "\n",
    "filtered_train_label_column = label_column_1[mask]\n",
    "\n",
    "print(\"Original shape:\", label_column_1.shape)\n",
    "print(\"Filtered shape:\", filtered_train_label_column.shape) #for labels of the files which were not found\n",
    "\n",
    "train_label_indices = one_hot_to_indices(filtered_train_label_column)\n",
    "convert_to_svm_format(train_summed_array, train_label_indices, \"train.svm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test dataset\n",
    "base_dir = \"iaprtc12/annotations_complete_eng/\"\n",
    "test_file=\"IAPRTC/iapr_test_list.txt\"\n",
    "\n",
    "\n",
    "test_summed_array,files_not_found=process_text_file_to_get_embedding(base_dir,test_file)\n",
    "\n",
    "\n",
    "test_label_data = loadmat(\"IAPRTC/IAPRTC-12_TestLabels.mat\")\n",
    "I_z_te=test_label_data['I_z_te']\n",
    "label_test_column_1 = I_z_te[0,0]  \n",
    "label_test_column_2 = I_z_te[0, 1] \n",
    "\n",
    "print(\"Column 1 Shape:\", label_test_column_1.shape)\n",
    "\n",
    "# label_test_column_1=label_test_column_1[:300]\n",
    "\n",
    "mask = np.ones(label_test_column_1.shape[0], dtype=bool)\n",
    "mask[files_not_found] = False\n",
    "\n",
    "filtered_test_label_column = label_test_column_1[mask]\n",
    "\n",
    "print(\"Original shape:\", label_test_column_1.shape)\n",
    "print(\"Filtered shape:\", filtered_test_label_column.shape) #for labels of the files which were not found\n",
    "\n",
    "test_label_indices = one_hot_to_indices(filtered_test_label_column)\n",
    "convert_to_svm_format(test_summed_array, test_label_indices, \"test.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
